{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMTNNSihuu8WM+l2ZBWweO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/B-Sumanth/2203A51551_NLP/blob/main/Textprocessing_1551.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
        "    return text\n",
        "corpus =[\n",
        "    \"Automated essay scoring (AES) is the use of specialized\"\n",
        "    \"computer programs to assign grades to essays written \"\n",
        "    \"in an educational setting.\"\n",
        "]\n",
        "cleaned_corpus = [clean_text(doc) for doc in corpus]\n",
        "print(cleaned_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bskdItS1UQy3",
        "outputId": "0a52f0ea-b01a-42b7-d773-92b4a86bf7c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['automated essay scoring aes is the use of specializedcomputer programs to assign grades to essays written in an educational setting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenized_corpus = [word_tokenize(doc) for doc in cleaned_corpus]\n",
        "print(tokenized_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQSVHjFXVXRP",
        "outputId": "27b92e5e-912b-4a79-eda1-c6fd5cf2198b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['automated', 'essay', 'scoring', 'aes', 'is', 'the', 'use', 'of', 'specializedcomputer', 'programs', 'to', 'assign', 'grades', 'to', 'essays', 'written', 'in', 'an', 'educational', 'setting']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_corpus = [[word for word in doc if word not in stop_words] for doc in tokenized_corpus]\n",
        "print(filtered_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-2dRerXVfPq",
        "outputId": "71c9a3d9-c8d6-43f9-bb2a-5b9a6bedc0f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['automated', 'essay', 'scoring', 'aes', 'use', 'specializedcomputer', 'programs', 'assign', 'grades', 'essays', 'written', 'educational', 'setting']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_corpus = [[stemmer.stem(word) for word in doc] for doc in filtered_corpus]\n",
        "lemmatized_corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in filtered_corpus]\n",
        "print(stemmed_corpus)\n",
        "print(lemmatized_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPdxjpoWVlOI",
        "outputId": "4fa87d8d-b548-4886-e3e6-7a46995f56dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['autom', 'essay', 'score', 'ae', 'use', 'specializedcomput', 'program', 'assign', 'grade', 'essay', 'written', 'educ', 'set']]\n",
            "[['automated', 'essay', 'scoring', 'aes', 'use', 'specializedcomputer', 'program', 'assign', 'grade', 'essay', 'written', 'educational', 'setting']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions # Install the contractions module\n",
        "import contractions # Import the contractions module\n",
        "\n",
        "expanded_corpus = [contractions.fix(doc) for doc in cleaned_corpus]\n",
        "print(expanded_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icb5s5RvV5iO",
        "outputId": "aa40943b-989a-4778-feb3-eff71334bc5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "['automated essay scoring aes is the use of specializedcomputer programs to assign grades to essays written in an educational setting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "expanded_corpus = [contractions.fix(doc) for doc in cleaned_corpus]\n",
        "print(expanded_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv5hF0oMVp3m",
        "outputId": "a201f1c8-54a4-49db-9217-ad1d3f492c61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['automated essay scoring aes is the use of specializedcomputer programs to assign grades to essays written in an educational setting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker # Change to from pyspellchecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "corrected_corpus = [[spell.correction(word) for word in doc] for doc in tokenized_corpus]\n",
        "print(corrected_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVZXDtDyWK_a",
        "outputId": "7d0454c7-4470-4a7e-9012-de88c0c8fd57"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n",
            "[['automated', 'essay', 'scoring', 'as', 'is', 'the', 'use', 'of', None, 'programs', 'to', 'assign', 'grades', 'to', 'essays', 'written', 'in', 'an', 'educational', 'setting']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "corrected_corpus = [[spell.correction(word) for word in doc] for doc in tokenized_corpus]\n",
        "print(corrected_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoYOWtjiWBrn",
        "outputId": "76f899a3-35fa-41b1-e27d-351ba9b830ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['automated', 'essay', 'scoring', 'as', 'is', 'the', 'use', 'of', None, 'programs', 'to', 'assign', 'grades', 'to', 'essays', 'written', 'in', 'an', 'educational', 'setting']]\n"
          ]
        }
      ]
    }
  ]
}